<html>

<head>
<meta http-equiv="Content-Type"
content="text/html; charset=iso-8859-1">
<meta name="GENERATOR" content="Microsoft FrontPage Express 2.0">
<title>The Virtual Machine</title>
</head>

<body bgcolor="#FFFFFF">
<h3 align="right"><a href="virtualmachine.html">&lt;== Previous</a> |	
 <a href="toc.html">Contents</a> |
 <a href="virtualmachine3.html"><strong>Next ==&gt;</strong></a></h3>
<h2 align="left">The Virtual Machine (cont.)</h2>

<h3 align="left">A Micro Benchmark</h3>

<p align="left">To illustrate the impact of adaptive optimization
on Smalltalk performance, let's look at a very small
micro-benchmark. A micro-benchmark is too small to predict
performance very accurately, however, it can be used to
demonstrate the basic points, and show the compilation process in
action.</p>

<p align="center"><smappl visual="
	| b v |
	b := ClassOutliner for: (ClassMirror on: Test).
    v := b topVisualWithHRule: false.
    b openSide: true selector: #simpleTest:.
	v := v withBorder: (Border standard3DRaised: true).
    v
"><br>
<strong>A micro-benchmark in the Test class</strong> <a
doit="| b v |
	b := ClassOutliner for: (ClassMirror on: Test).
    v := b topVisualWithHRule: false.
    b openSide: true selector: #simpleTest:.
    v launch">(spawn)</a></p>

<p align="left">The above micro-benchmark, #simpleArray:, simply
loops 10 million times, each time storing an integer into an
array. A really good optimizer could probably even optimize away
the actual store itself, but Strongtalk does not currently do
this, so the benchmark really does do the work it appears to. Run
the benchmark by clicking on the following doIt: <a
doit="Test benchmark: [ Test simpleTest: 10000000 ]">Test
benchmark: [ Test simpleTest: 10000000 ]</a>. The benchmark is
run 10 times, with the number of milliseconds for each run
displayed in the Transcript, followed by the best time. I
intentionally did not use any type annotations in this benchmark,
so that it is clear that the type-system is not used for
optimization.</p>

<p align="left">The first thing to notice is that after one or
two runs, the runs are dramatically faster. This is because
initially the code is not determined to be performance-critical,
so it is run by the interpreter. At some point, it becomes
compiled, and subsequent runs are much faster. The compiler
actually runs in the middle of the first run or two, but the
current version of the VM doesn't use the compiled version until
the next run, which is why it is important to run benchmarks
multiple times. It is technically possible for the VM to switch
to fully optimized code in the middle of the loop, but that work
was not completed on this VM.</p>

<p align="left">On my machine, an Intel Pentium III (Tualatin)
running Windows XP at 1Ghz, this benchmark produced a best time
of 100 milliseconds. Under VisualWorks V5i.4 (non-commercial),
which does dynamic translation to native code and is the fastest
previous implementation of Smalltalk (as far as I know), it
produced a best time of 456ms. This is a speedup factor of 4.6.
As mentioned before, micro-benchmarks are not that accurate, and
this is probably an overstatement of the actual speedup that
Strongtalk would produce for a real program. On a large, more
representative set of benchmarks, we computed a number of years ago that the
Strongtalk speedup was approximately 3.5 over VisualWorks, on
Smalltalk code written in a normal style, although this has not been rechecked recently.</p>

<h4 align="left">Adding Sends and a Block Closure</h4>

<p align="left">Now, let's do something more interesting. One of
the main points we have been making is that not only is
Strongtalk much faster in an absolute sense, but that it
dramatically reduces the <em>relative cost</em> of writing
well-structured code (i.e. code that is more finely factored and
uses block closures freely to implement custom control
structures). So let's add some message sends and a block closure
to our benchmark, and see what the effect on the performance is.</p>

<p align="left">Open the method #notSoSimpleTest: in the browser
above. Follow its execution path down into #fancyStoreIntoArray:
and then into #evaluateBlock:. You can see it does the same basic
computation that #simpleTest: did, but it moves the array store
down into a block closure in another method, that is then passed
to yet another method and finally evaluated. In other Smalltalks,
this adds a large amount of additional work, because not only do
we have three additional message sends, but we are forcing a
block closure (of the <em>copying</em> type) to be created in the
intervening method, which normally requires an actual closure
object to be allocated <em>every</em> time #fancyStoreIntoArray:
is called (to hold the array reference). This is a big cost in
performance-critical code. Here is a doIt that will run the
#notSoSimpleTest: <a
doit="Test benchmark: [ Test notSoSimpleTest: 10000000 ]">Test
benchmark: [ Test notSoSimpleTest: 10000000 ]</a>.</p>

<p align="left">On my machine, VisualWorks runs this new
benchmark in 1232ms, which is 270% slower than the first version
of the benchmark. Strongtalk runs it in 136ms, which is only 36%
slower than the first version of the benchmark. And in fact, with
a small amount of improvement to the Strongtalk code generator,
it should be able to run this benchmark without any slowdown <em>at
all</em>, since the adaptive optimizer is already completely
eliminating the additional message sends and the block closure.</p>

<h3 align="right"><a href="virtualmachine3.html"><strong>The
Virtual Machine, cont. ==&gt;</strong></a></h3>
</body>
</html>
